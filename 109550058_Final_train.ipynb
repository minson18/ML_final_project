{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0ydvP90xs5Imi5sOHiAor",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minson18/ML_final_project/blob/main/109550058_Final_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下這段 pip install 執行完要按下輸出區的 RESTART RUNTIME，再繼續往下執行。"
      ],
      "metadata": {
        "id": "E-Eo81rcknXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOOQKCQ9jPWO"
      },
      "outputs": [],
      "source": [
        "pip install imbalanced-ensemble"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "# Root Path\n",
        "os.chdir('/content/drive/MyDrive/ML/final project')\n",
        "\n",
        "TRAIN_PATH = \"train.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J137gZh7klG0",
        "outputId": "8acb22a9-c3f8-4ae5-b6dd-c1a70f24b16e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import random\n",
        "#from imbalanced_ensemble.ensemble import OverBoostClassifier\n",
        "#from imbalanced_ensemble.ensemble import SelfPacedEnsembleClassifier\n",
        "#from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "x8iNwQWWlQK-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-proccessing Training data\n",
        "train_df = pd.read_csv(TRAIN_PATH) # modify train pat\n",
        "\n",
        "# translate attribute_0, attribute_1 from string to int\n",
        "train_df['attribute_0'] = train_df['attribute_0'].replace(\n",
        "            ['material_7', 'material_5'], [7, 5])\n",
        "train_df['attribute_1'] = train_df['attribute_1'].replace(\n",
        "            ['material_8', 'material_5', 'material_6'], [8, 5, 6])\n",
        "\n",
        "# add column measurement_5_wasmissing, measurement_4_wasmissing\n",
        "train_df.insert(1, 'measurement_5_wasmissing', 0)\n",
        "train_df.loc[pd.isna(train_df['measurement_5']), 'measurement_5_wasmissing'] = 1\n",
        "train_df.insert(1, 'measurement_4_wasmissing', 0)\n",
        "train_df.loc[pd.isna(train_df['measurement_4']), 'measurement_4_wasmissing'] = 1\n",
        "\n",
        "# drop redundant columns\n",
        "train_df.drop(['id', 'product_code'], axis = 1, inplace = True)\n",
        "# fill Nan\n",
        "train_df = train_df.fillna(0)\n",
        "\n",
        "# first dataset with selected features\n",
        "train_df_select = train_df[[\n",
        "        \"loading\", \"measurement_17\", \"measurement_6\", \"measurement_4\",\n",
        "        \"measurement_2\", \"measurement_5_wasmissing\", \"attribute_3\", \n",
        "        \"attribute_1\", \"measurement_9\", \"measurement_4_wasmissing\", \n",
        "        \"measurement_7\", \"attribute_0\", \"failure\"]]\n",
        "trainData_select = train_df_select.to_numpy()\n",
        "x_train_select = trainData_select[:, :-1]\n",
        "y_train_select = trainData_select[:, -1]\n",
        "\n",
        "# second dataset with all features\n",
        "trainData = train_df.to_numpy()\n",
        "x_train = trainData[:, :-1]\n",
        "y_train = trainData[:, -1]"
      ],
      "metadata": {
        "id": "v_T5Ns39l2HL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from imbalanced_ensemble.ensemble import RUSBoostClassifier\n",
        "\n",
        "def train(x_train, y_train, x_train_select, y_train_select):\n",
        "  # Below are model_select_A, model_select_B which \n",
        "  # using selected features dataset\n",
        "  model_select_A = RUSBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1), \n",
        "    n_estimators=110, learning_rate= 0.052, random_state=10)\n",
        "  \n",
        "  model_select_A.fit(x_train_select, y_train_select)\n",
        "  pickle.dump(model_select_A, open(\"model_select_A.pickle\", 'wb'))\n",
        "  \n",
        "  model_select_B = RUSBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1), \n",
        "    n_estimators=105, learning_rate= 0.051, random_state=34)\n",
        "  \n",
        "  model_select_B.fit(x_train_select, y_train_select)\n",
        "  pickle.dump(model_select_B, open(\"model_select_B.pickle\", 'wb'))\n",
        "\n",
        "  # Below are model_missing_A, model_missing_B which \n",
        "  # using all features dataset\n",
        "  model_A = RUSBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1), \n",
        "    n_estimators=105, learning_rate= 0.049, random_state=89)\n",
        "  \n",
        "  model_A.fit(x_train, y_train)\n",
        "  pickle.dump(model_A, open(\"model_A.pickle\", 'wb'))\n",
        "  \n",
        "  model_B = RUSBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1), \n",
        "    n_estimators=100, learning_rate= 0.051, random_state=11)\n",
        "  \n",
        "  model_B.fit(x_train, y_train)\n",
        "  pickle.dump(model_B, open(\"model_B.pickle\", 'wb'))"
      ],
      "metadata": {
        "id": "YUtOpVyPm-cr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(x_train, y_train, x_train_select, y_train_select)"
      ],
      "metadata": {
        "id": "Ynlx686gvCuW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Below is doing hyperparameter search using Kfokd"
      ],
      "metadata": {
        "id": "WMqQwcXUjPAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from imbalanced_ensemble.ensemble import CompatibleBaggingClassifier\n",
        "from imbalanced_ensemble.ensemble import RUSBoostClassifier\n",
        "from imbalanced_ensemble.sampler.under_sampling import RandomUnderSampler \n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def train_XGB(\n",
        "    x_train, y_train, n_estimators=110, learning_rate=0.071, random_state=0, max_depth=1, \n",
        "    gamma=0, subsample=0.6, colsample_bytree=1, sampler_random_state=0):\n",
        "  \n",
        "  model = XGBClassifier(\n",
        "    n_estimators=n_estimators, learning_rate=learning_rate, \n",
        "    random_state=random_state, max_depth=max_depth, gamma=gamma, \n",
        "    subsample=subsample, colsample_bytree=colsample_bytree)\n",
        "  \n",
        "  #sampler = ADASYN(random_state=sampler_random_state)\n",
        "  #X_res, y_res = sampler.fit_resample(x_train, y_train)\n",
        "  #model.fit(X_res, y_res)\n",
        "  model.fit(x_train, y_train)\n",
        "  return model\n",
        "\n",
        "def train_boost(\n",
        "    x_train, y_train, n_estimators=100, learning_rate=0.051, \n",
        "    random_state=11, max_depth=1):\n",
        "\n",
        "  model = RUSBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=max_depth), \n",
        "    n_estimators=n_estimators, learning_rate= learning_rate, \n",
        "    random_state=random_state)\n",
        "  \n",
        "  model.fit(x_train, y_train)\n",
        "  return model\n",
        "    \n",
        "def train_bagging(\n",
        "    x_train, y_train, n_estimators=100, learning_rate=1.0, \n",
        "    max_samples=1, max_features=1, random_state=0):\n",
        "  \n",
        "  #print(np.unique(y_train).size)\n",
        "  model = CompatibleBaggingClassifier(\n",
        "    n_estimators=n_estimators, max_samples=max_samples, \n",
        "    max_features=max_features, random_state=random_state)\n",
        "  \n",
        "  model.fit(x_train, y_train)\n",
        "  return model\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QMGcdx7KkAWd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "kfold_data= []\n",
        "for i, (train_index, val_index) in enumerate(kf.split(x_train)):\n",
        "  kfold_data.append([train_index, val_index])\n",
        "\n",
        "def grid_search(x_train, y_train, n_estimators_list = [100], lr_list = [1], k = 5):\n",
        "\n",
        "  accuracy_all = []\n",
        "  for n_estimators in n_estimators_list:\n",
        "    accuracy_raw = []\n",
        "\n",
        "    for lr in lr_list:\n",
        "      accuracy = []\n",
        "\n",
        "      for val in range(k):        \n",
        "        train_index = kfold_data[val][0]\n",
        "        test_index = kfold_data[val][1]\n",
        "\n",
        "        model = train_boost(\n",
        "          x_train[train_index], y_train[train_index], \n",
        "          n_estimators=n_estimators, learning_rate=lr, random_state=None)\n",
        "        \n",
        "        val_accuracy = model.score(x_train[test_index], y_train[test_index])\n",
        "        accuracy.append(val_accuracy)\n",
        "\n",
        "      print(f\"n_estimators={n_estimators}, lr={lr} : Accuracy:{sum(accuracy) / len(accuracy)}\")\n",
        "      accuracy_raw.append(sum(accuracy) / len(accuracy))\n",
        "\n",
        "    accuracy_all.append(accuracy_raw)\n",
        "  return np.array(accuracy_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_-w9DMuJaYjy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "n_estimators_list = [80, 85, 90, 95, 100, 105, 110, 115, 120]\n",
        "lr_list = [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0001]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "J8x7jzj5eSu8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "accuracy_result = grid_search(\n",
        "  x_train, y_train, n_estimators_list = n_estimators_list, \n",
        "  lr_list = lr_list, k = 5)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wEJawgK6eEQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "accuracy_result_select = grid_search(\n",
        "  x_train_select, y_train_select, \n",
        "  n_estimators_list = n_estimators_list, \n",
        "  lr_list = lr_list, k = 5)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "92W5k7bleOYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "def heatmap(data, row_labels, col_labels, ax=None, cbar_kw=None, cbarlabel=\"\", **kwargs):\n",
        "    \n",
        "  if ax is None:\n",
        "    ax = plt.gca()\n",
        "\n",
        "  if cbar_kw is None:\n",
        "    cbar_kw = {}\n",
        "\n",
        "  # heatmap\n",
        "  im = ax.imshow(data, **kwargs)\n",
        "\n",
        "  # create colorbar\n",
        "  cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
        "  cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\", fontsize=18)\n",
        "\n",
        "  # set ticks and labels\n",
        "  ax.set_xticks(np.arange(data.shape[1]))\n",
        "  ax.set_yticks(np.arange(data.shape[0]))\n",
        "  ax.set_xticklabels(col_labels)\n",
        "  ax.set_yticklabels(row_labels)\n",
        "  \n",
        "  return im, cbar\n",
        "\n",
        "\n",
        "def annotate_heatmap(im, valfmt=\"{x:.2f}\",  textcolors=(\"black\", \"white\"), **textkw):\n",
        "  \n",
        "  data = im.get_array()\n",
        "\n",
        "  threshold = im.norm(data.max())/2.\n",
        "\n",
        "  # Set default alignment to center, but allow it to be overwritten by textkw.\n",
        "  kw = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
        "  kw.update(textkw)\n",
        "\n",
        "  # formation of string\n",
        "  if isinstance(valfmt, str):\n",
        "    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
        "\n",
        "  # create text and set text colors\n",
        "  texts = []\n",
        "  #print(data)\n",
        "  for i in range(data.shape[0]):\n",
        "    for j in range(data.shape[1]):\n",
        "      kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
        "      text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
        "      texts.append(text)\n",
        "\n",
        "    return texts\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wt0vew5te3Ow"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "im, cbar = heatmap(\n",
        "  accuracy_result, n_estimators_list, lr_list, ax=ax, \n",
        "  cmap=\"GnBu\", cbarlabel=\"accuracy\")\n",
        "\n",
        "texts = annotate_heatmap(im, valfmt=\"{x:.3f}\")\n",
        "plt.title(\"Hyperparameter Gridsearch\", fontsize = 20)\n",
        "plt.xlabel('learning rate', fontsize=18)\n",
        "plt.ylabel('n_estimators', fontsize=18)\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "h7U2dp8afoGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "im, cbar = heatmap(\n",
        "  accuracy_result, n_estimators_list, lr_list, ax=ax, \n",
        "  cmap=\"GnBu\", cbarlabel=\"accuracy\")\n",
        "\n",
        "texts = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
        "plt.title(\"Hyperparameter Gridsearch Selected\", fontsize = 20)\n",
        "plt.xlabel('learning rate', fontsize=18)\n",
        "plt.ylabel('n_estimators', fontsize=18)\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iGvO5BDcfA8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Below using kaggle api to submit preidictions"
      ],
      "metadata": {
        "id": "Df5ZR-oojCsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload \"kaggle.json\", which is kagggle's API\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ackeUuNAi2Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MGewKGdxikeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "test_path = \"test.csv\"\n",
        "\n",
        "# Pre-proccessing Testing data\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# translate attribute_0, attribute_1 from string to int\n",
        "test_df['attribute_0'] = test_df['attribute_0'].replace(\n",
        "  ['material_7', 'material_5'], [7, 5])\n",
        "test_df['attribute_1'] = test_df['attribute_1'].replace(\n",
        "  ['material_7', 'material_5', 'material_6'],[7, 5, 6])\n",
        "\n",
        "# add column measurement_5_wasmissing, measurement_4_wasmissing\n",
        "test_df.insert(1, 'measurement_5_wasmissing', 0)\n",
        "test_df.loc[pd.isna(test_df['measurement_5']),'measurement_5_wasmissing'] = 1\n",
        "test_df.insert(1, 'measurement_4_wasmissing', 0)\n",
        "test_df.loc[pd.isna(test_df['measurement_4']),'measurement_4_wasmissing'] = 1\n",
        "\n",
        "# drop redundant columns\n",
        "test_df = test_df.drop(['product_code'], axis=1)\n",
        "# fill Nan\n",
        "test_df = test_df.fillna(0)\n",
        "\n",
        "# first dataset with selected features\n",
        "test_df_select = test_df[[\n",
        "  \"loading\", \"measurement_17\", \"measurement_6\", \"measurement_4\", \"measurement_2\", \n",
        "  \"measurement_5_wasmissing\", \"attribute_3\", \"attribute_1\", \"measurement_9\", \n",
        "  \"measurement_4_wasmissing\", \"measurement_7\", \"attribute_0\"\n",
        "  ]]\n",
        "x_test_select = test_df_select.to_numpy()\n",
        "\n",
        "# second dataset with all features\n",
        "testData = test_df.to_numpy()\n",
        "x_test = testData[:, 1:]\n",
        "\n",
        "test_id = testData[:, 0].astype(int)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uIH5649zgzkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(\n",
        "    x_train, y_train, x_test, n_estimators=100, learning_rate=0.051, \n",
        "    random_state=0, max_depth=1, max_samples=1, max_features=1, gamma=0, \n",
        "    subsample=1, colsample_bytree=1, sampler_random_state=0, comment=\"all data\"):\n",
        "  \n",
        "  \"\"\"\n",
        "  my_model = train_bagging(\n",
        "    x_train, y_train, n_estimators=n_estimators, \n",
        "    max_samples=max_samples, max_features=max_features, random_state=random_state)\n",
        "  my_model = train_XGB(\n",
        "    x_train, y_train, n_estimators=n_estimators, learning_rate=learning_rate, \n",
        "    random_state=random_state, max_depth=max_depth, gamma=gamma, subsample=subsample, \n",
        "    colsample_bytree=colsample_bytree, sampler_random_state=sampler_random_state)\n",
        "  \"\"\"\n",
        "  \n",
        "  my_model = train_boost(\n",
        "    x_train, y_train, n_estimators=n_estimators, \n",
        "    learning_rate=learning_rate, random_state=random_state, \n",
        "    max_depth=1)\n",
        "  \n",
        "  y_pred = my_model.predict_proba(x_test)\n",
        "  y_pred = y_pred[:, 1]\n",
        "  submit = np.array([test_id, y_pred]).T\n",
        "\n",
        "  submit = submit.tolist()\n",
        "  for sub in submit:\n",
        "    sub[0] = int(sub[0])\n",
        "    sub[1] = float(sub[1])\n",
        "\n",
        "  csv_writer = csv.writer(open('submission.csv', 'w', newline=''))\n",
        "  csv_writer.writerow([\"id\", \"failure\"])\n",
        "  csv_writer.writerows(submit)\n",
        "  csv_writer = csv.writer(open('submission.csv', 'w', newline=''))\n",
        "  csv_writer.writerow([\"id\", \"failure\"])\n",
        "  csv_writer.writerows(submit)\n",
        "  \n",
        "  \"\"\"\n",
        "  message = f\"CompatibleBaggingClassifier(n_estimators={n_estimators}, max_samples={max_samples}, \n",
        "    max_features={max_features}, random_state={random_state})\"\n",
        "  message = f\"XGBClassifier(n_estimators={n_estimators}, learning_rate={learning_rate}, \n",
        "    random_state={random_state}, max_depth={max_depth}, gamma={gamma}, subsample={subsample}, \n",
        "    colsample_bytree={colsample_bytree})\"\n",
        "  \"\"\"\n",
        "\n",
        "  message = f\"\"\"RUSBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth={max_depth}), \n",
        "    n_estimators={n_estimators}, learning_rate={learning_rate}, random_state={random_state}), \n",
        "    with {comment}\"\"\"\n",
        "  cmd = f\"\"\"kaggle competitions submit -c tabular-playground-series-aug-2022 -f submission.csv -m '{message}'\"\"\"\n",
        "  os.system(cmd)"
      ],
      "metadata": {
        "id": "DEXnFw7Zfqjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for n_estimators in [95, 100, 105, 110]:\n",
        "  for lr in [0.048, 0.049, 0.050, 0.051, 0.052]:\n",
        "    for random_state in random.sample(range(0, 100), 20):\n",
        "      search(\n",
        "        x_train, y_train, x_test, n_estimators=n_estimators, \n",
        "        random_state=random_state, learning_rate=lr, \n",
        "        comment=\"feature selected\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ouxbhAuAgBuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for n_estimators in [95, 100, 105, 110]:\n",
        "  for lr in [0.048, 0.049, 0.050, 0.051, 0.052]:\n",
        "    for random_state in random.sample(range(0, 100), 20):\n",
        "      search(\n",
        "        x_train_select, y_train_select, x_test_select, \n",
        "        n_estimators=n_estimators, learning_rate=lr, \n",
        "        random_state=random_state)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dovIGMJDf20J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}